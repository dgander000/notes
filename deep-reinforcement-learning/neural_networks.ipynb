{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "# Neural Network Architecture\n",
    "----"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Combination of linear models    \n",
    "![](./images/neural_networks/combining_regions.png)   \n",
    "\n",
    "### Calculate the probability for each model - Add them  - Apply sigmoid function   \n",
    "![](./images/neural_networks/combining_regions_sigmoid.png)  \n",
    "\n",
    "### Each linear model can be weighted and a bias can be added  \n",
    "![](./images/neural_networks/weighted_combination.png)  \n",
    "\n",
    "### Shown as a neural network  \n",
    "![](./images/neural_networks/neural_network_0.png)  \n",
    "\n",
    "![](./images/neural_networks/neural_network_1.png)  \n",
    "\n",
    "![](./images/neural_networks/neural_network_2.png)  \n",
    "\n",
    "![](./images/neural_networks/neural_network_3.png)  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Deep Neural Networks\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Neural network consists of an input layer - hidden layer - output layer   \n",
    "Can have multiple input nodes   \n",
    "Can have multiple output nodes (multi-class classification)   \n",
    "![](./images/neural_networks/basic_neural_network.png)  \n",
    "\n",
    "### Deep neural networks have multiple hidden layers   \n",
    "Linear models combine to create non-linear models and these combine to create even more non-linear models   \n",
    "![](./images/neural_networks/deep_neural_network.png)\n",
    "\n",
    "### Many hidden layers can be combined to learn very complex functions  \n",
    "![](./images/neural_networks/deeper_neural_network.png)   \n",
    "\n",
    "### Multi-Class classification can be done by having more output nodes    \n",
    "![](./images/neural_networks/multi_class_classification.png)   "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Feedforward\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Get output prediction from input vector  \n",
    "Take input vector - apply a sequence of linear models and sigmoid functions   \n",
    "When combined become a highly non-linear map   \n",
    "\n",
    "![](./images/neural_networks/feedforward_0.png)   \n",
    "![](./images/neural_networks/feedforward_1.png) \n",
    "![](./images/neural_networks/feedforward_multi_layer.png)   \n",
    "\n",
    "### Error function    \n",
    "![](./images/neural_networks/error_function.png) "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Backpropagation\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Training the neural network   \n",
    "Gradient Descent   \n",
    "Calculate error and its gradient   \n",
    "Move in direction of negative of gradient to reduce error    \n",
    "Error is fed back, after feedforward, to update the weights   \n",
    "\n",
    "![](./images/neural_networks/backpropagation_0.png)   \n",
    "![](./images/neural_networks/backpropagation_1.png)  \n",
    "![](./images/neural_networks/backpropagation_2.png)  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Overfitting and Underfitting\n",
    "\n",
    "### Underfitting  \n",
    "too simple of a model  \n",
    "error due to bias\n",
    "\n",
    "### Overfitting \n",
    "too complex of a model    \n",
    "too specific - does not fit generalize   \n",
    "error due to variance  \n",
    "\n",
    "### How to choose a model   \n",
    "Error on the side of overly complicated models and apply techniques to prevent overfitting   \n",
    "\n",
    "![](./images/neural_networks/overfitting_underfitting.png)  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Early stopping\n",
    "Start with random weights - stop training when train and test error are small and before they start diverging  \n",
    "\n",
    "y-axis is error, dotted line is testing error, solid line is training error   \n",
    "\n",
    "![](./images/neural_networks/early_stopping.png)   "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Regularization\n",
    "Large coeffients lead to overfitting, even though the predictions may be better, so penalize large coeffients   \n",
    "Activation function can become too squished and result in problems with gradient descent   \n",
    "Points classified as incorrect will generate large errors and make it tough to tune the model   \n",
    "\n",
    "![](./images/neural_networks/regularization_0.png)   \n",
    "\n",
    "![](./images/neural_networks/regularization_1.png)   \n",
    "\n",
    "![](./images/neural_networks/regularization_2.png)   \n",
    "\n",
    "![](./images/neural_networks/regularization_3.png)   "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Dropout\n",
    "Some parts of the network may have very large weights and end up dominating training   \n",
    "Randomly turn off some of the nodes   \n",
    "Give the algorithm a paramater - the probability each node will be turned off during an epoch (for example 0.2)   \n",
    "\n",
    "![](./images/neural_networks/dropout.png)   "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Local Minima\n",
    "Gradient descent can get stuck at local minima   \n",
    "\n",
    "![](./images/neural_networks/local_minima.png)   "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Random Restart\n",
    "\n",
    "One way to combat local minima   \n",
    "Start from random places and do gradient descent from all of them   "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Momentum\n",
    "\n",
    "Another way to combat local minima   \n",
    "\n",
    "![](./images/neural_networks/momentum.png)  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Vanishing Gradient\n",
    "Derivative of sigmoid function can be small as you get further from 0   \n",
    "Gets worse when you multiply multiply of these - leads to a tiny number   \n",
    "\n",
    "![](./images/neural_networks/vanishing_gradient_0.png)    \n",
    "\n",
    "![](./images/neural_networks/vanishing_gradient_1.png)  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Other Activation Functions\n",
    "One way to address the vanishing gradient problem is to use a different activation function   \n",
    "Note y should be 0, not 0.5    \n",
    "\n",
    "![](./images/neural_networks/tanh.png)  \n",
    "\n",
    "![](./images/neural_networks/relu.png)  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Stochastic Gradient Descent\n",
    "\n",
    "Use subset of data (batch) and run each step (epoch)   \n",
    "So run several batches instead of one big batch   \n",
    "Each step will be less accurate but better overall   "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Learning Rate   \n",
    "General rule - if your model isn't Learning decrease the Learning rate   \n",
    "Best learning rates decrease as the model is getting closer to the solution   \n",
    "\n",
    "If steep: long steps   \n",
    "If plain: small steps  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}