{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,), (0.5,)),\n",
    "                             ])\n",
    "# Download and load the training data\n",
    "trainset = datasets.MNIST('data/MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = datasets.MNIST('data/MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for our network\n",
    "input_size = 784\n",
    "hidden_sizes = [128, 64]\n",
    "output_size = 10\n",
    "\n",
    "# Build a feed-forward network\n",
    "model = nn.Sequential(OrderedDict([\n",
    "                      ('fc1', nn.Linear(input_size, hidden_sizes[0])),\n",
    "                      ('relu1', nn.ReLU()),\n",
    "                      ('fc2', nn.Linear(hidden_sizes[0], hidden_sizes[1])),\n",
    "                      ('relu2', nn.ReLU()),\n",
    "                      ('logits', nn.Linear(hidden_sizes[1], output_size))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 1/3...  Loss: 2.2867\n",
      "Epoch: 1/3...  Loss: 2.2697\n",
      "Epoch: 1/3...  Loss: 2.2481\n",
      "Epoch: 1/3...  Loss: 2.2276\n",
      "Epoch: 1/3...  Loss: 2.2081\n",
      "Epoch: 1/3...  Loss: 2.1821\n",
      "Epoch: 1/3...  Loss: 2.1545\n",
      "Epoch: 1/3...  Loss: 2.1294\n",
      "Epoch: 1/3...  Loss: 2.0921\n",
      "Epoch: 1/3...  Loss: 2.0590\n",
      "Epoch: 1/3...  Loss: 2.0249\n",
      "Epoch: 1/3...  Loss: 1.9781\n",
      "Epoch: 1/3...  Loss: 1.9230\n",
      "Epoch: 1/3...  Loss: 1.8799\n",
      "Epoch: 1/3...  Loss: 1.8125\n",
      "Epoch: 1/3...  Loss: 1.7528\n",
      "Epoch: 1/3...  Loss: 1.6956\n",
      "Epoch: 1/3...  Loss: 1.6270\n",
      "Epoch: 1/3...  Loss: 1.5477\n",
      "Epoch: 1/3...  Loss: 1.4974\n",
      "Epoch: 1/3...  Loss: 1.3966\n",
      "Epoch: 1/3...  Loss: 1.3474\n",
      "Epoch: 1/3...  Loss: 1.2821\n",
      "Epoch: 2/3...  Loss: 0.6762\n",
      "Epoch: 2/3...  Loss: 1.1801\n",
      "Epoch: 2/3...  Loss: 1.1149\n",
      "Epoch: 2/3...  Loss: 1.0791\n",
      "Epoch: 2/3...  Loss: 1.0362\n",
      "Epoch: 2/3...  Loss: 0.9853\n",
      "Epoch: 2/3...  Loss: 0.9820\n",
      "Epoch: 2/3...  Loss: 0.9454\n",
      "Epoch: 2/3...  Loss: 0.9040\n",
      "Epoch: 2/3...  Loss: 0.8613\n",
      "Epoch: 2/3...  Loss: 0.8155\n",
      "Epoch: 2/3...  Loss: 0.8206\n",
      "Epoch: 2/3...  Loss: 0.7691\n",
      "Epoch: 2/3...  Loss: 0.7616\n",
      "Epoch: 2/3...  Loss: 0.7403\n",
      "Epoch: 2/3...  Loss: 0.7193\n",
      "Epoch: 2/3...  Loss: 0.6798\n",
      "Epoch: 2/3...  Loss: 0.6864\n",
      "Epoch: 2/3...  Loss: 0.6782\n",
      "Epoch: 2/3...  Loss: 0.6518\n",
      "Epoch: 2/3...  Loss: 0.6329\n",
      "Epoch: 2/3...  Loss: 0.6401\n",
      "Epoch: 2/3...  Loss: 0.6251\n",
      "Epoch: 3/3...  Loss: 0.0614\n",
      "Epoch: 3/3...  Loss: 0.5858\n",
      "Epoch: 3/3...  Loss: 0.6053\n",
      "Epoch: 3/3...  Loss: 0.5885\n",
      "Epoch: 3/3...  Loss: 0.5478\n",
      "Epoch: 3/3...  Loss: 0.5703\n",
      "Epoch: 3/3...  Loss: 0.5524\n",
      "Epoch: 3/3...  Loss: 0.5277\n",
      "Epoch: 3/3...  Loss: 0.5622\n",
      "Epoch: 3/3...  Loss: 0.5461\n",
      "Epoch: 3/3...  Loss: 0.5184\n",
      "Epoch: 3/3...  Loss: 0.5457\n",
      "Epoch: 3/3...  Loss: 0.5331\n",
      "Epoch: 3/3...  Loss: 0.5042\n",
      "Epoch: 3/3...  Loss: 0.4932\n",
      "Epoch: 3/3...  Loss: 0.4892\n",
      "Epoch: 3/3...  Loss: 0.5075\n",
      "Epoch: 3/3...  Loss: 0.5189\n",
      "Epoch: 3/3...  Loss: 0.4891\n",
      "Epoch: 3/3...  Loss: 0.4826\n",
      "Epoch: 3/3...  Loss: 0.4609\n",
      "Epoch: 3/3...  Loss: 0.4601\n",
      "Epoch: 3/3...  Loss: 0.4700\n",
      "Epoch: 3/3...  Loss: 0.4744\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "print_every = 40\n",
    "steps = 0\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in iter(trainloader):\n",
    "        steps += 1\n",
    "        # Flatten MNIST images into a 784 long vector\n",
    "        images.resize_(images.size()[0], 784)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward and backward passes\n",
    "        output = model.forward(images)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if steps % print_every == 0:\n",
    "            print(\"Epoch: {}/{}... \".format(e+1, epochs),\n",
    "                  \"Loss: {:.4f}\".format(running_loss/print_every))\n",
    "            \n",
    "            running_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}