{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Networks\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In RL an agent outputs an action and the environment returns an observation (state of the system) and reward\n",
    "- Through interaction the agent learns the best action to take for each state (Q-table) based on long term expected reward\n",
    "- Q-table is not feasible for large/continuous/non-linear environments  \n",
    "- Deep RL uses non-linear function approximators (DNN) to calculate action values directly from environemnt feedback\n",
    "- Represented as Deep Neural Networks \n",
    "- Use Deep Learning to find the optimal paramaters   \n",
    "\n",
    "[Issues in Using Function Approximation for Reinforcement Learning](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.73.3097&rep=rep1&type=pdf)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepMind's DQN\n",
    "[Human-level control through deep reinforcement learning](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf)   \n",
    "- pass in video game images\n",
    "- outputs a vector of action values with max value being the action to take\n",
    "- game score at each time step was the reinforcement signal \n",
    "- converted frames to grayscale and converted to 84 x 84\n",
    "- stacked 4 frames together so state space size was 84 x 84 x 4 (to help with sequential data)\n",
    "- produces a Q value for each action in a single forward pass - to exploit take one with max value\n",
    "- neural network architecture\n",
    "    - convolutional layers (spatial relationships, spatial rule space, temporal properties across frames)\n",
    "    - 3 convolution layers with ReLU\n",
    "    - 1 fully connected hidden layer with ReLU\n",
    "    - 1 fully connected linear output layer the produced the action value vector \n",
    "- not guarenteed to converge on optimal value function\n",
    "- network weigths can oscillate or diverge due to the high correlation between actions and states\n",
    "- RL is nortoriously unstable when neural networks are used to represent action values  \n",
    "- ways to overcome this\n",
    "    - Experience Replay\n",
    "    - Fixed Q Targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience Replay\n",
    "- rolling history of past data - replay pool\n",
    "- behavior distribution is averaged over many previous states smoothing out learning and avoiding oscillations\n",
    "- experience tuple (S<sub>t</sub>, A<sub>t</sub>, R<sub>t+1</sub>, S<sub>t+1</sub>)\n",
    "- typical online Q-learning learns from the experience tuple then discards it and moves on\n",
    "- instead store these experiences\n",
    "- some states are rare to come by and some actions can be very costly so be able to recall those experiences\n",
    "- Replay Buffer - storage for each experience tuple\n",
    "- sample small batch from replay buffer to learn\n",
    "- can learn from individual tuple multiple times, recall rare experiences and make better use of experience\n",
    "- sequence of experience tuples can be highly correlated so learning in sequential order can be swayed the effects of this correlation\n",
    "- can sample from replay buffer at random, break correlation and prevent action values from oscillating or diverging\n",
    "- reduces RL to a SL scenario\n",
    "- experience replay is sampling a small batch of experience tuples from the replay buffer to learn from"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixed Q-Targets\n",
    "- target network to represent old Q-function which is used to compute the loss of every action during training\n",
    "- at each step of training the Q-function values change and the value estimates can spiral out of control\n",
    "- allows more reliable convergence\n",
    "- Q-learning is a form of Temporal-Difference (TD) learning\n",
    "- update a guess with a guess\n",
    "- when updating weights there is a correlation between target and parameters being changed - like trying to hit moving target\n",
    "- fix the function parameters used to generate the target, w<sup>-</sup>, copy of w that isn't changed during learning step\n",
    "![weight update](images/drl/fixed_Q_target.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-Learning Algorithm\n",
    "![deep Q learning](images/drl/deep_Q_learning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double DQN\n",
    "[Deep Reinforcement Learning with Double Q-learning](https://arxiv.org/pdf/1509.06461.pdf)   \n",
    "\n",
    "- Deep Q-Learning tends to overestimate action values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prioritized Experience Replay  \n",
    "[Prioritized Experience Replay](https://arxiv.org/abs/1511.05952)   \n",
    "\n",
    "- perhaps an angent can learn more effectively from certain transitions over others\n",
    "- important transitions should be sampled with higher probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dueling DQN\n",
    "[Dueling Network Architectures for Deep Reinforcement Learning](https://arxiv.org/abs/1511.06581)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Improvements\n",
    "[Asynchronous Methods for Deep Reinforcement Learning](https://arxiv.org/abs/1602.01783)  \n",
    "[A Distributional Perspective on Reinforcement Learning](https://arxiv.org/abs/1707.06887)  \n",
    "[Noisy Networks for Exploration](https://arxiv.org/abs/1706.10295)   \n",
    "[Rainbow: Combining Improvements in Deep Reinforcement Learning](https://arxiv.org/abs/1710.02298)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}