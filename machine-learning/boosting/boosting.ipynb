{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Learning: Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a bunch of simple rules and combining them into a more complex rule that works very well. Similar to decision trees, we build them step by step.\n",
    "\n",
    "- Learn over a subset 1 of data -> generate rule  \n",
    "- Learn over a subset 2 of data -> generate different rule   \n",
    "...   \n",
    "...  \n",
    "- Learn over a subset n of data -> generate different rule   \n",
    "- Finally combine all rules into a **complex rule**   \n",
    "\n",
    "Simplest approach:  \n",
    "1. Choosing subsets: Uniformly randomly select data, apply a learner  \n",
    "2. Combine: assume each learner is equal and take mean   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Bagging  \n",
    "---\n",
    "<img src=\"../images/bagging.png\" width=600 align=\"left\"/>  \n",
    "5 subsets of 5 examples, chosen randomly and with replacement   \n",
    "\n",
    "Each learns a 3rd order polynomial   \n",
    "\n",
    "Combine by average (red line)  \n",
    "\n",
    "Compared to 4th order polynomial regression (blue line)  \n",
    "\n",
    "Regression does better on training set but bagging does better on testing (in this case)  \n",
    "\n",
    "Does better because less overfitting - works like cross validation - don't get trapped by a few points that are wrong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting  \n",
    "---\n",
    "- Instead of picking subsets randomly, take advantage of what we are learning as we go along and pick subsets containing the hardest examples  \n",
    "- Combine using weighted mean  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error  \n",
    "- Regression: squared difference between predicted and actual  \n",
    "- Classification: total number of mismatches over the total number of examples  \n",
    "\n",
    "But every example may not be of equal importance.  \n",
    "We should also consider the probability of encountering the example.  \n",
    "\n",
    "$\\textrm{P} \\left (h(x) \\ne c(x) \\right)$  \n",
    "Error: probabilty, given underlying distribution, that I'll disagree with the true concept given some instance x   \n",
    "\n",
    "Some instances may be rare.   \n",
    "Think about amount of time you may be wrong instead of the distinct number of mistakes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weak Learner\n",
    "- A learner that always does at least better than chance (no matter what the distribution is over the data)\n",
    "- Error rate will always be less than half  \n",
    "- Learner will always learn something   \n",
    "\n",
    "$\\forall_D \\textrm{P}_D ( h(x) \\ne c(x) ) \\le 1/2 - \\epsilon $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudocode \n",
    "Given Training Set:  ${(x_i, y_i)}$ with $y \\in ( -1, +1 )$ (binary classification)  \n",
    "\n",
    "Loop over some time step, $t$:   \n",
    "For t=1 to T:\n",
    "- construct distribution: $D_t$\n",
    "- find weak classifier, $h_t(x)$\n",
    "    - with small error\n",
    "    - $\\epsilon_t = \\textrm{P}_{D_t} ( h_t(x_i) \\ne y_i ) $\n",
    "\n",
    "output final hypothesis: $H_{\\textrm{final}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution Construction\n",
    "<img src=\"../images/boosting.png\" width=800 align=\"left\"/>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Hypothesis\n",
    "<img src=\"../images/boosting_final.png\" width=800 align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example\n",
    "<img src=\"../images/boosting_example1.png\" width=450 align=\"left\"/>\n",
    "<img src=\"../images/boosting_example2.png\" width=450 align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/dmlc/xgboost/tree/master/demo   \n",
    "https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting\n",
    "<img src=\"../images/boosting_error.png\" width=600 align=\"left\"/>   \n",
    "Need to think about confidence, not just right or wrong (error).   \n",
    "Confidence: how strongly you believe in the answer.\n",
    "\n",
    "As you add more weak learners the +'s and -'s move from boundary and it gets more confident but error stays the same.  And essentially creates a bigger and bigger margin.   \n",
    "\n",
    "Large margins tend to minimize overfitting.  \n",
    "\n",
    "Boosting will overfit if:\n",
    "- weak learning uses A.N.N. with many layers and nodes\n",
    "- essentially if the weak learner overfits and don't stop overfitting so will boosting  \n",
    "\n",
    "pink noise - uniform noise - BOOSTING MAY OVERFIT in this case  \n",
    "white noise - gaussian noise  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the discussion of boostings impact on *overfitting*, we've been ignoring some information. \n",
    "\n",
    "What we normally keep track of is:\n",
    "- *error* (i.e., the probability that you will come up with an answer that disagrees with your training set). \n",
    "\n",
    "however, what we are also keeping track of is *confidence*. \n",
    "\n",
    "$ H(x)_{\\text{final}} = \\text{sgn} \\left( \\sum_t \\alpha_t h_t(x) \\right)$\n",
    "\n",
    "which simply outputs the sign of the sum over the weak hypothes +1, - 1, or 0.\n",
    "\n",
    "Take the above and divide by the weights we used, normalizing the output:\n",
    "\n",
    "$$ \n",
    "H(x)_{\\text{final}} = \n",
    "\\text{sgn} \n",
    "\\left ( \n",
    "\\frac{\\sum_t\\alpha_t h_t(x) }{ \\sum\\alpha_t}\n",
    "\\right )\n",
    "$$\n",
    "\n",
    "This normalization reduces the output between -1 and +1, and in the process of adding more weak learners, the margin increases (e.g., the distance between boundaries). This increases the confidence while the actual error remains the same.\n",
    "\n",
    "Boosting tends to overfit also if the underlying weak learner tends to overfit, as in the case of an artificial neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
