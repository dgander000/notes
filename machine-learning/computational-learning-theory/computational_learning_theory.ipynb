{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational Learning Theory\n",
    "---\n",
    "- define learning problems (what we want the learning algorithm to do)\n",
    "- show that specific algorithms work or don't work\n",
    "- show some problems may be fundamentally hard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources in machine learning\n",
    "---\n",
    "- time\n",
    "- space \n",
    "- data (samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Inductive Learning \n",
    "---\n",
    "- Probability of successful training   \n",
    "    - may have bad or noisy data\n",
    "    - probability of success:  1 - $\\delta$\n",
    "- Number of training examples:  $m$\n",
    "- Complexity of hypothesis class\n",
    "    - hypothesis class that is too simple won't be able to represent much and learn anything complicated\n",
    "    - hypothesis class that is too complex will likely overfit\n",
    "- Accuracy to which target concept is approximated:  $\\epsilon$\n",
    "- Manner in which training examples presented (especially matters in online training as opposed to batch)\n",
    "    - batch training (fixed training set presented to learning algorithm)  \n",
    "    - online (one at a time)\n",
    "- Manner in which training examples selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Training Examples (learner/teacher)\n",
    "---\n",
    "- Learner ask questions of teacher\n",
    "    - learner selects training examples\n",
    "- Teacher gives examples to help learner\n",
    "    - teacher chooses x, gives c(x)\n",
    "- Fixed distribution\n",
    "    - x chosen from D by nature\n",
    "    - some natural fixed distribution\n",
    "- Evil distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is an inductive learning trying to do\n",
    "---\n",
    "- Trying to find the best hypothesis in some class $H$\n",
    "- Hypothesis all map inputs to outputs (e.g. yes/no)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teaching via 20 questions\n",
    "---\n",
    "<img src=\"../images/twenty_questions.png\" width=600 align=\"left\"/>  \n",
    "Teacher only needs to ask one question.  Knows the answer and can 'lead' to right answer by including answer in question.  \n",
    "\n",
    "Learner doesn't know.  Best to hope for is splitting things roughly in half.  Log base 2 time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teacher with constrained queries\n",
    "---  \n",
    "<img src=\"../images/teacher_constrained.png\" width=600 align=\"left\"/>  \n",
    "$X: \\{x_1 x_2 ... x_k\\}$  k-bit input (input space)   \n",
    "$H:$ conjunctions of literal or negation (hypothesis class)   \n",
    "A hypothesis in this set,   $h$: $\\overset{-}{x_2}$ and $x_4$ and $\\overset{-}{x_5}$   \n",
    "\n",
    "To solve:    \n",
    "\n",
    "Show what is irrelevant... two positive examples which are unperturbed by a changing feature.   \n",
    "    \n",
    "Show what is relevant... k negative examples which validate that perturbing a relevant feature matters.   \n",
    "    \n",
    "Even though there are 3^k (positive, absent, negated) hypotheses, the smart teacher can ask k + 2 questions.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learner with constrained queries\n",
    "---\n",
    "<img src=\"../images/learner_constrained.png\" width=600 align=\"left\"/>  \n",
    "Does not know the actual answer like the teacher does so does not know what the right training examples are.\n",
    "\n",
    "Start enumerating every data point from 0,...,0 to 1,...,1 which is $2^k$ possibilities.  Negative results don't help.  The first positive result is what helps.   \n",
    "\n",
    "This is a evil example with only one positive result:  \n",
    "$x_1$ and $\\overset{-}{x_2}$ and $x_3$ and $x_4$ and $\\overset{-}{x_5}$  \n",
    "\n",
    "If learner could use 20 questions trick it would take up to $log_2 3k$ = $klog_2 3$  \n",
    "But can't, needs to go through all combinations $2^k$, which is exponential time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learner with mistake bounds\n",
    "---\n",
    "1. Assume each feature positive and negated.\n",
    "2. Given input, compute output.\n",
    "3. If wrong, set all positive features that were 0 to absent, negative features that were 1 to abset. Go to (2).\n",
    "\n",
    "Never make more than k + 1 mistakes.  \n",
    "<img src=\"../images/learner_with_mistake_bounds1.png\" width=470 align=\"left\"/>  \n",
    "<img src=\"../images/learner_with_mistake_bounds2.png\" width=500 align=\"right\"/>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions\n",
    "---\n",
    "Learner chooses examples  \n",
    "Teacher chooses examples  \n",
    "Nature chooses examples - most interesting and relevant   \n",
    "Mean teacher chooses examples  \n",
    "\n",
    "- **Computational complexity**: how much computational effort is needed for learner to \"converge\" to the answer   \n",
    "- **Sample complexity**: *batch learning* - how many training examples are needed for a learner to create a successful hypothesis\n",
    "- **Mistake bounds**: *online learning* - how many misclassifications can a learner make over an infinite run?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version Space\n",
    "---\n",
    "- True hypothesis: true hypothesis trying to learn - *true concept* - $c$ in $H$ \n",
    "- Training set: learning from training set $S$ (subset of possible inputs)  \n",
    "    - consists of those examples along with the true class for all those X's   \n",
    "- Candidate hypothesis: at any point in time a learner may have a candidate hypotheis $h$ in $H$\n",
    "- Consisten learner: produces $h$ such that $c(x)$ = $h(x)$ for $x$ in $S$ (always produces a hypothesis that is consistent with data)\n",
    "- Version space: space of all hypothesis consistent with data\n",
    "    - set of hypothesis in the hypothesis space such that they are consistent with the data\n",
    "\n",
    "<img src=\"../images/version_spaces.png\" width=540 align=\"left\"/>   \n",
    "<img src=\"../images/version_spaces2.png\" width=430 align=\"right\"/>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PAC Learning - error of h\n",
    "---\n",
    "**Training error**: fraction of training examples misclassified by $h$  \n",
    "- $h$ could be different from target concept  \n",
    "- target concept may have a training error of 0 but some other hypothesis may have some training error   \n",
    "\n",
    "**True error**: fraction of training examples that would be misclassified on a sample drawn from $D$   \n",
    "- essentially infinite limit  \n",
    "- probability that some sample drawn from $D$ would be misclassified by $h$  \n",
    "\n",
    "$error_D(h) = Pr_{x from D}[c(x) != h(x)]$    \n",
    "\n",
    "Captures notion that it's okay to misclassify examples you will never see.   \n",
    "Examples you rarely see will have a small error. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PAC Learning (probably approximately correct)\n",
    "---\n",
    "- c: concept class\n",
    "    - set from which the concept we're trying to learn comes from\n",
    "- L: learner\n",
    "- H: hypothesis space\n",
    "- n: |H|, size of hypothesis space\n",
    "- D: distribution over inputs\n",
    "- 0 <= $\\epsilon$ <= 1/2 (our error goal)  \n",
    "    - error in hypotheis produced no bigger than $\\epsilon$ (epsilon)  \n",
    "- 0 <= $\\delta$ <= 1/2 (certainty goal -- with probability $1 - \\delta$ the algorithm has to work, must produce true error less than $\\epsilon$)\n",
    "\n",
    "PAC -- probably $(1 - \\delta)$ approximately $(\\epsilon)$ correct $(error_D(h) = 0)$\n",
    "\n",
    "C is PAC-learnable by L using H iff learner L will, with probability $1 - \\delta$, output a hypothesis h in H such that $error_D(h) <= \\epsilon$ in time and samples *polynomial* in $1/\\epsilon$, $1/\\delta$, and $n$.  \n",
    "\n",
    "\n",
    "Probably:    $1 - \\delta$        \n",
    "Approximately: $\\epsilon$    \n",
    "Correct:  $h(x)=c(x)$ or $error_D(h) = 0$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\epsilon$-exhausted version space\n",
    "---\n",
    "$VS(S)$ is $\\epsilon$-exhausted iff for all $h$ in VS(S), $error_D(h)$ <= $\\epsilon$   \n",
    "\n",
    "A version space derived from a particular sample is considered epsilon exhausted if and only if for all the hypothesis that are in that version space they have low error.  \n",
    "\n",
    "Every hypothesis in the version space has error less than epsilon.   \n",
    "\n",
    "<img src=\"../images/epsilon_exhausted_quiz.png\" width=485 align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Haussler Theorem - bound true error\n",
    "---\n",
    "\n",
    "Let $error_D(h_1,...,h_k \\in H) > \\epsilon$ -- this is high true error.\n",
    "\n",
    "How much data do we need to knock out these hypotheses?\n",
    "\n",
    "$Pr_{x from D}(h_i(x) = c(x)) <= 1 - \\epsilon$   \n",
    "\n",
    "$Pr(h_i$ consistent with c on m examples$) <= (1 - \\epsilon)^m$   \n",
    "\n",
    "Pr(at least one of h1,...,hk consistent with c on m examples) <= $k(1 - \\epsilon)^m <= |H|(1 - \\epsilon)^m$  \n",
    "\n",
    "Note that $(1 - \\epsilon)^m <= exp(\\epsilon*m)$ (this comes from $-\\epsilon >= ln(1 - \\epsilon)$)\n",
    "\n",
    "So we have Pr(at least one of hypothesis is consistent with c on m examples) <= $Hexp(-\\epsilon) <= \\delta$  \n",
    "\n",
    "This is the upperbound that version space is not $\\epsilon$-exhausted after m samples. We want $\\delta$ to be a bound on this.\n",
    "\n",
    "$ln |H| - \\epsilon * m <= ln \\delta m >= 1/\\epsilon (ln |H| + ln (1/\\delta))$ (i.e. polynomial)\n",
    "\n",
    "Satisfying this will satisfy PAC-learnability.  \n",
    "\n",
    "\n",
    "$m >= (1/\\epsilon) (ln |H| + ln (1/\\delta))$ (i.e. polynomial)    \n",
    "- this assumed target in hypothesis\n",
    "- otherwise agnostic and the bound is slightly different, but still polynomial.\n",
    "- infinite hypothesis space can be a problem\n",
    "\n",
    "\n",
    "<img src=\"../images/haussler_theorm.png\" width=485 align=\"left\"/>   \n",
    "<img src=\"../images/haussler_theorm2.png\" width=485 align=\"right\"/>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/pac_example.png\" width=485 align=\"left\"/>\n",
    "$H = {h_i(x) = x_i}$ (where x is 10-bits) $\\epsilon = 0.1 \\delta = 0.2 D$: uniform   \n",
    "\n",
    "1/0.1 * (ln 10 + ln (1/0.2)) = 10*(ln 10 + ln 5) = 10*ln 50 = 39  \n",
    "\n",
    "This bound is agnostic to distribution of nature's data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infinite Hypothesis Space\n",
    "---\n",
    "Bounding number of samples we need to learn a classifier or concept in some hypothesis class H   \n",
    "okay as long as number of samples (m) is $m >= (1/\\epsilon) (ln |H| + ln (1/\\delta))$  \n",
    "Number of samples depends on size of hypothesis space, what if you have a very large hypothesis space?  \n",
    "\n",
    "Which hypothesis spaces are infinite?   \n",
    "- linear separators (yes)  \n",
    "- artificial neural networks (yes)\n",
    "- decision trees (discrete inputs) (no)\n",
    "- decision trees (continuous inputs) (yes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Power of a hypothesis space\n",
    "---\n",
    "\n",
    "What is the largest set of inputs that the hypothesis class can label in all possible ways?   \n",
    "  \n",
    "In the above hypothesis, there is no way for a pair of data points to be labeled in all possible ways. It can however label one point is all possible ways (two ways).   \n",
    "\n",
    "<img src=\"../images/hypothesis_space1.png\" width=485 align=\"left\"/>   \n",
    "<img src=\"../images/hypothesis_space2.png\" width=485 align=\"right\"/>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VC dimension\n",
    "---\n",
    "What is the largest set of inputs that the hypothesis class can label in all possible ways?  \n",
    "\n",
    "Shattering == label in all possible ways  \n",
    "\n",
    "Vapnik-Chervonenkis dimension -- the largest set of inputs shattered by a class of learners   \n",
    "Amount of data needed to learn - as long as VC dimension is finite, even if hypothesis class is finite, can still know how much data needed to  learn    \n",
    "\n",
    "<img src=\"../images/vc_dimension1.png\" width=485 align=\"left\"/>   \n",
    "<img src=\"../images/vc_dimension2.png\" width=485 align=\"right\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Ring\n",
    "---\n",
    "VC dimension often number of parameters   \n",
    "\n",
    "- one dimension: 1 ($\\theta$)\n",
    "- interval: 2 (a, b)\n",
    "- two dimension: 3 (w, $\\theta$) (w is vector of size 2)\n",
    "- three dimensiion: 4\n",
    "- d-dimensional hyperplane: d + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample complexity and VC dimension\n",
    "---\n",
    "<img src=\"../images/vc1.png\" width=485 align=\"left\"/>   \n",
    "\n",
    "Update Haussler's theorem with VC dimension:\n",
    "\n",
    "$m >= 1/\\epsilon (8* VC(H)log_2(13/\\epsilon) + 4log_2(2/\\delta))$ (infinite case)  \n",
    "\n",
    "$m >= 1/\\epsilon (ln |H| + ln (1/\\delta))$ (finite case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VC dimension of finite H\n",
    "---\n",
    "<img src=\"../images/vc2.png\" width=485 align=\"left\"/>    \n",
    "\n",
    "Upper bound: $d = VC(H)$ implies there exist $2^d$ distinct concepts (each gets a different h) $2^d$ <= $|H|$ and $d <= log_2|H|$\n",
    "\n",
    "Fundamental Theorem of Machine Learning H is PAC-learnable if and only if VC dimension is finite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
