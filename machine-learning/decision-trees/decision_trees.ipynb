{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representation   \n",
    "---\n",
    "Ask a series of questions, moving down a tree (starting at top/root), until you get to some output (answer).    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logical structure containing:      \n",
    "- Nodes\n",
    "\t- decision nodes\n",
    "\t- pick a particular attribute and ask a question about it (raining?)\n",
    "- Edges\n",
    "\t- represent values from decision node (yes/no)\n",
    "- Leaves\n",
    "\t- answer\n",
    "\t- end states representing the output\n",
    "\n",
    "### Goal   \n",
    "Ask questions to keep narrowing down possibilities until you get to an answer.   \n",
    "Usefulness of a question depends on the answer from the previous question and how much the question narrows the space of possibilities.    \n",
    "\n",
    "### Example    \n",
    "This would be a simple candidate concept for classifying animals.     \n",
    "<img src=\"../images/simpledt.png\" align=\"left\"/>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expressiveness  \n",
    "---\n",
    "Decision trees can be used to represent logic conditions such as AND, OR or XOR.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/dt_and.png\" width=200 align=\"left\"/>   \n",
    "**AND** (any)\n",
    "- Has linear complexity when mapped to a decision tree  \n",
    "- Easy because any (A or B) can result in False (e.g, 2 attributes only need 2 nodes)  \n",
    "- For $n$ number of nodes, there will be $n$ operations, $O(n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/dt_or.png\" width=200 align=\"left\"/>   \n",
    "**OR** (any)\n",
    "- Has linear complexity when mapped to a decision tree  \n",
    "- Easy because any (A or B) can result in True (e.g, 2 attributes only need 2 nodes)  \n",
    "- For $n$ number of nodes, there will be $n$ operations, $O(n)$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/dt_xor.png\" width=200 align=\"left\"/>   \n",
    "**XOR** (parity)\n",
    "- Has exponential complexity when mapped to a decision tree  \n",
    "- Hard because all nodes need to be evaluated (parity) (e.g, 2 attributes need 3 nodes)  \n",
    "- For $n$ nodes, there is a bound of $2^n -1$ operations, $O(2^n)$   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general in ML, we hope to encounter problems that are more like *any* vs *parity*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How expressive is a decision tree?   \n",
    "If we have to search over all possible decision trees to find the best one, how many decision trees do we need to worry about?   \n",
    "\n",
    "**XOR** example   \n",
    "- $n$ boolean attributes  \n",
    "- output is boolean   \n",
    "\n",
    "How many trees?  \n",
    "Looking at the truth table  \n",
    "- All combinations need to be evaluated  \n",
    "- $2^n$ rows, where n is number of attributes   \n",
    "- $2^m$ outputs, where m is number of row combinations (from above)  \n",
    "- number of trees is $2^{2^n}$   \n",
    "for n=6, number of trees is 1.844674407×10¹⁹   \n",
    "\n",
    "Decision Trees are extremeley expressive, allowing for an extremeley large hypothosis space. We must have clever ways to focus search using algorithims.   \n",
    "<img src=\"../images/dt_xor_exp.png\" align=\"left\"/>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm   \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple algorithm  \n",
    "1. Pick best attribute (best splits the data)\n",
    "2. Ask question   \n",
    "3. Follow path of answer   \n",
    "4. Go to 1 (repeat until you get answer)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example best attribute   \n",
    "Best attribute and question to ask.  Perfectly splits the data.  \n",
    "<img src=\"../images/dt_best_attribute.png\" align=\"left\"/>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ID3 algorithm   \n",
    "Top down learning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Psuedocode  \n",
    "Loop:\n",
    "\n",
    " - $A$ $\\leftarrow$ best attribute\n",
    " - Assign $A$ as decision attribute for *node*\n",
    " - For each value of $A$ create a descendent of *node*.\n",
    " - Sort training examples to *leaves*.\n",
    " - IF Examples perfectly classified $\\rightarrow$ **STOP**\n",
    " - ELSE Iterate over leaves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best Attribute\n",
    "*Information Gain*   \n",
    "One way to define best attribute is information gain.  The amount of information gained by picking a particular attribute.   \n",
    "\n",
    "Mathmatically, information gain quantifies the reduction in randomness over the labels you have with a set of data, based upon knowing the value of a particular attribute.  \n",
    "\n",
    "It is defined as:   \n",
    "$$\\textrm{Gain}(S, A) = \\textrm{Entropy}(S) - \\sum_v \\frac{\\left|S_v\\right|}{\\left|S\\right|} \\cdot \\textrm{Entropy}(S_v) $$ where:   \n",
    "\n",
    "- $S$ is the set of training examples    \n",
    "- $A$ is an attribute.    \n",
    "    \n",
    "Entropy is defined as:  \n",
    "$$  Entropy(S) = - \\sum_v p(v)\\log(v) $$      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ID3's biases:    \n",
    "1. Good Splits at the top of the tree\n",
    "2. Correct over Incorrect\n",
    "3. Prefers shorter trees vs. longer (due to bias #1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous attributes\n",
    "- Check certain ranges   \n",
    "- With continuous attributes, it can make sense to repeat asking about an attribute along a path in a decision tree.  You are essentially asking a different question. Ask a different question, not the same. (e.g., is age > 50, later maybe need to ask if age < 20)   \n",
    "For discrete attributes it does not make sense to repeat asking about the attribute. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to stop  \n",
    "- When everything is classified correctly\n",
    "- When we've run out of attributes\n",
    "- When overfitting starts to occur (tree is too big, too complicated)  \n",
    "    - Use cross validation to stop expanding tree when error is low enough   \n",
    "    - Prune to reduce decition tree size (vote)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Regression\n",
    "- Splitting: variance?\n",
    "- Output: average, local linear fit  \n",
    "\n",
    "TODO more details on decision tree regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources   \n",
    "https://github.com/xbno/Projects/blob/master/Models_Scratch/Decision%20Trees%20from%20scratch.ipynb    \n",
    "https://github.com/xbno/Projects/blob/master/Models_Scratch/Random%20Forest%20from%20scratch.ipynb   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1372, 5)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = '~/data/bank-note/'\n",
    "file_name = 'data_banknote_authentication.txt'\n",
    "file = file_path + file_name\n",
    "df = pd.read_csv(file, names=['variance','skewness','kurtosis','entropy','class'])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variance</th>\n",
       "      <th>skewness</th>\n",
       "      <th>kurtosis</th>\n",
       "      <th>entropy</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.62160</td>\n",
       "      <td>8.6661</td>\n",
       "      <td>-2.8073</td>\n",
       "      <td>-0.44699</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.54590</td>\n",
       "      <td>8.1674</td>\n",
       "      <td>-2.4586</td>\n",
       "      <td>-1.46210</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.86600</td>\n",
       "      <td>-2.6383</td>\n",
       "      <td>1.9242</td>\n",
       "      <td>0.10645</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.45660</td>\n",
       "      <td>9.5228</td>\n",
       "      <td>-4.0112</td>\n",
       "      <td>-3.59440</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.32924</td>\n",
       "      <td>-4.4552</td>\n",
       "      <td>4.5718</td>\n",
       "      <td>-0.98880</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   variance  skewness  kurtosis  entropy  class\n",
       "0   3.62160    8.6661   -2.8073 -0.44699      0\n",
       "1   4.54590    8.1674   -2.4586 -1.46210      0\n",
       "2   3.86600   -2.6383    1.9242  0.10645      0\n",
       "3   3.45660    9.5228   -4.0112 -3.59440      0\n",
       "4   0.32924   -4.4552    4.5718 -0.98880      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data):\n",
    "    \"\"\"\n",
    "    @summary: normalize data to a value between 0 and 1 \n",
    "    @param data: np.ndarray \n",
    "    @returns: normalized np.ndarray \n",
    "    \"\"\"\n",
    "    # norm = (x - x_min) / (x_max - x_min)\n",
    "    x_max = data.max()\n",
    "    x_min = data.min()\n",
    "    return (data - x_min) / (x_max - x_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  3.6216 ,   8.6661 ,  -2.8073 ,  -0.44699],\n",
       "       [  4.5459 ,   8.1674 ,  -2.4586 ,  -1.4621 ],\n",
       "       [  3.866  ,  -2.6383 ,   1.9242 ,   0.10645],\n",
       "       ...,\n",
       "       [ -3.7503 , -13.4586 ,  17.5932 ,  -2.7771 ],\n",
       "       [ -3.5637 ,  -8.3827 ,  12.393  ,  -1.2823 ],\n",
       "       [ -2.5419 ,  -0.65804,   2.6842 ,   1.1952 ]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array(df.iloc[:, :-1])\n",
    "y = np.array(df.iloc[:, -1])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.54872005, 0.70785003, 0.34591883, 0.42037539],\n",
       "       [0.57787732, 0.69211842, 0.35691866, 0.3883535 ],\n",
       "       [0.55642971, 0.35124998, 0.49517515, 0.43783379],\n",
       "       ...,\n",
       "       [0.31617167, 0.00992098, 0.98945758, 0.3468715 ],\n",
       "       [0.32205801, 0.17004148, 0.825416  , 0.39402533],\n",
       "       [0.35429094, 0.41371776, 0.51914954, 0.47217867]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_norm = normalize(X)\n",
    "X_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold\n",
      "Accuracy:       0.9680232558139535\n",
      "Accuracy:       0.9912790697674418\n",
      "Accuracy:       0.9766081871345029\n",
      "Accuracy:       0.9736842105263158\n",
      "Shuffle Split\n",
      "Accuracy:       0.9927272727272727\n",
      "train test split\n",
      "Accuracy:       0.9818181818181818\n"
     ]
    }
   ],
   "source": [
    "kfold = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\n",
    "shufflesplit = StratifiedShuffleSplit(n_splits=1, random_state=42, test_size=.2)\n",
    "\n",
    "dt = tree.DecisionTreeClassifier()\n",
    "print(\"KFold\")\n",
    "for train_index, test_index in kfold.split(x, y):\n",
    "    # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    # Train the model\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    dt.fit(X_train, y_train)\n",
    "    y_pred = dt.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print('Accuracy:      ', accuracy)\n",
    "    \n",
    "dt = tree.DecisionTreeClassifier()\n",
    "print(\"Shuffle Split\")\n",
    "for train_index, test_index in shufflesplit.split(x, y):\n",
    "    # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    # Train the model\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    dt.fit(X_train, y_train)\n",
    "    y_pred = dt.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print('Accuracy:      ', accuracy)\n",
    "    \n",
    "dt = tree.DecisionTreeClassifier()\n",
    "print(\"train test split\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred = dt.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy:      ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
