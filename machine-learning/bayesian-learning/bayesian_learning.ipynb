{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Goal $\\rightarrow$ Learn the best hypothesis given data and some domain knowledge\n",
    "    - replace best with most probable\n",
    "- Learn the most probable hypothesis H given data and domain knowledge. (best == most probable)\n",
    "    - probability of some particular hypothesis drawn from some hypothesis class given some amount of data (D)\n",
    "    - $P_r(h | D)$ \n",
    "\n",
    "Trying to find the hypothesis $h$ with highest probability $P_r \\rightarrow$   $argmax_{h \\in H} Pr(h | D)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes Rule\n",
    "---\n",
    "Allows you to swap \"causes\" and \"effect\".  \n",
    "\n",
    "$P_r(h|D) = {\\dfrac{P_r(D|h)P_r(h)}{P_r(D)}}$   \n",
    " \n",
    "- $P_r(h|D) \\rightarrow$  probability of a specific hypothesis given input data (hypothesis that best fits the data)  \n",
    "- $P_r(D|h) \\rightarrow$  probability of data given the hypothesis (hypothesis that describes the data the best)   \n",
    "    - classification example: traing data is a set of inputs $x_i$ and labels $d_i$ associated with those inputs $D = [x_i, d_i]$\n",
    "    - it’s the likelihood of seeing some particular labels associated with input points, given a world where some hypothesis h is true (really trying to understand labels associated with the x's)  \n",
    "    - in a world where the hypothesis is true... likelihood of seeing this data   \n",
    "    - given some x's, what's the probability of seeing some label - like running the hypothesis  \n",
    "- $P_r(D) \\rightarrow$  prior on the data (prior belief of seeing some particular data)   \n",
    "    - just ends up being a normalizing term and does not always matter   \n",
    "- $P_r(h) \\rightarrow$  prior on a particular h drawn from the hypothesis space \n",
    "    - encapsulates our prior belief that one hypothesis is likely or unlikely compared to other hypotheses\n",
    "    - this encapsulate our domain domain knowledge         \n",
    "\n",
    "Deriving Bayes Rule (chain rule):   \n",
    "$P_r(a,b) = P_r(a|b) P_r(b)$ and... $P_r(a,b) = P_r(b|a) P_r(a)$   \n",
    "\n",
    "$P_r(a|b) P_r(b)$ = $P_r(b|a) P_r(a)$   \n",
    "\n",
    "$P_r(a|b) =  {\\dfrac{P_r(b|a) P_r(a)}{P_r(b)}}$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes Rule Example\n",
    "---\n",
    "Test has accuracy:  \n",
    "Positive result:  98% true positive (2% false positive)    \n",
    "Negative result:  97% true negative (3% false negative)\n",
    "\n",
    "Only 0.8 percent of population has condition.  \n",
    "\n",
    "c = Condition, t = Positive test result   \n",
    "probability have condition given positive test result =     \n",
    "(probability of positive result given you have condition * prior probability of having condition) / probability of positive test result   \n",
    "probability of positive test result = (probability of true positive * prior probability of having condition) + (probability of false negative * prior probability of having condition)   \n",
    "\n",
    "$P_r(C|T) = {\\dfrac{P_r(P|T)P_r(C)}{P_r(P)}}$ = ${\\dfrac{0.98 \\times 0.008}{0.0376}}$ ~ 0.209  \n",
    "\n",
    "\n",
    "probability don't have condition given positive test result =   \n",
    "(probability of positive result given you don't have condition * prior probability of not having condition) / probability of positive test result   \n",
    "NOTE: use 3% since looking at negative results in this case (not C)\n",
    " \n",
    "$P_r(not C|T) = {\\dfrac{P_r(T | not C)P_r(not C)}{P_r(T)}}$ = ${\\dfrac{0.03 \\times 0.992}{0.0376}}$ ~ 0.791  \n",
    "\n",
    "If 0.008 were higher, the test would be more useful.  \n",
    "Priors matter!!!  The probability of actually having the condition based on a positive test result is brought down by the extremely low prior probability of having the condition.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Learning Algorithm\n",
    "---\n",
    "For each $h \\in H \\longrightarrow$  Calculate $P_r(h|D) = {\\dfrac{P_r(D|h)P_r(h)}{P_r(D)}}$  \n",
    "\n",
    "Output $\\longrightarrow h = argmax_{h \\in H} Pr(h | D)$   \n",
    "\n",
    "\n",
    "- Since we’re interested in finding the hypothesis with the highest probability, not the exact probability value for each hypothesis, we can ignore the normalization factor $P_r(D)$.  \n",
    "\n",
    "$$P_r(h|D) \\approx P_r(D|h)P_r(h)$$  \n",
    "\n",
    "- Using this approximate probability, we can calculate the Maximum a Posteriori:  \n",
    "\n",
    "$$h_{map} = argmax_{h \\in H} Pr(h | D)P_r(h)$$  \n",
    "\n",
    "- We can also drop $P_r(h)$ from the equation, ending up with the Maximum Likelihood:   \n",
    "\n",
    "$$h_{ml} = argmax_{h \\in H} Pr(D | h)$$   \n",
    "\n",
    "- We’re not exactly dropping $P_r(h)$, we just assume that our prior belief is all hypothesis are equally likely.  This is called a Uniform Prior.  Don't have a strong prior.  Determining $P_r(h)$ can be difficult.  \n",
    "- Direct computation not practical for large hypothesis spaces.  Have to look at all h."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Learning in Action\n",
    "---\n",
    "Classification example  \n",
    "\n",
    "Assume:\n",
    "- We have labeled training data ${<x_i, d_i>}$ as noise-free examples of c\n",
    "- $c \\in H$  (true concept c is in the hypothesis space)  \n",
    "- Uninform prior over hypothesis space (no reason to believe any h is more likely than another)\n",
    "\n",
    "Need to calculate $P_r(h|D)$\n",
    "\n",
    "$$P_r(h|D) = {\\dfrac{P_r(D|h)P_r(h)}{P_r(D)}}$$  \n",
    "\n",
    "$P_r(h) = {\\dfrac{1}{|H|}}$    \n",
    "\n",
    "$P_r(D|h)$ = 1 if $d_i = h(x_i)$ for all i; 0 otherwise  (= 1 if h is in VS(D))  \n",
    "\n",
    "$P_r(D) = \\sum_{h_i \\in H} P_r(D|h_i)P_r(h_i) = \\sum_{h_i \\in VS_{H,D}} 1 * {\\dfrac{1}{|H|}} = {\\dfrac{|VS|}{|H|}}$\n",
    "\n",
    "$P_r(h|D) = {\\dfrac{{1 * \\dfrac{1}{|H|}}}{{\\dfrac{|VS|}{|H|}}}} = {\\dfrac{1}{|VS|}}$ \n",
    "\n",
    "This means, given a bunch of data, the probability of a particular hypothesis being correct is uniform over elements of the version space.\n",
    "\n",
    "Any element of the version space is good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Learning with Noise\n",
    "---\n",
    "- Assume:\n",
    "    - We have labeled training data ${<x_i, d_i>}$\n",
    "    - $d_i = f(x_i) + \\epsilon_i$ (where $\\epsilon_i$ is error)\n",
    "    - $e_i \\approx N(0, \\sigma^2)$ IID (Independent and Identically Distributed)\n",
    "        - normal distribution, 0 mean, some variance    \n",
    "\n",
    "\n",
    "- What is the maximum likelihood hypothesis?  Assuming uniform prior.   \n",
    "$h_{ml} = argmax_{h \\in H} P_r(h | D) = argmax_{h \\in H} P_r(D | h)$      \n",
    "      \n",
    "      \n",
    "- Take the product of the probability of each data point given the hypothesis:    \n",
    "$h_{ml} = argmax_{h \\in H} \\prod_{i} P_r(d_i | h)$    \n",
    "    \n",
    "    \n",
    "- This probability can be approximated with Gaussian (assuming Gaussian noise):   \n",
    "$h_{ml} = argmax_{h \\in H} \\prod_{i} {\\dfrac{1}{\\sqrt{2 \\pi \\sigma^2}}} e^{{\\frac{{\\frac{-1}{2}}(d_i-h(x_i))^2}{\\sigma^2}}} $    \n",
    "\n",
    "\n",
    "- Since we're looking for the maximum we can simplify:  \n",
    "     - remove ${\\frac{1}{\\sqrt{2 \\pi \\sigma^2}}}$ since it doesn't depend on i and has no effect\n",
    "     - take natural log $\\ln$ to remove exponential (since it's monotonic) (log of product is same as sum of logs)  \n",
    "     $h_{ml} = argmax_{h \\in H} \\sum_{i} {\\frac{{\\frac{-1}{2}} (d_i - h(x_i))^2}{\\sigma^2}}$   \n",
    "     - the ${\\frac{-1}{2}}$ and $\\sigma^2$ can go (take min since getting rid of minus sign)        \n",
    "     $h_{ml} = argmin_{h \\in H} \\sum_{i} {(d_i - h(x_i))^2}$   \n",
    "\n",
    "This means: If you’re looking for the maximum likelihood hypothesis, you should minimize the sum of\n",
    "squared error.\n",
    "\n",
    "This is the sum of squared error.... derived from the gaussian noise model and the maximum likelihood assumption.    \n",
    "\n",
    "Making assumption that x's don't have error, probably not likely.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Learning:  Minimum Description Length\n",
    "---\n",
    "Best hypothesis is one that maximizes:    \n",
    "\n",
    " \n",
    "$h_{map} = argmax_{h \\in H} P_r(D | h) P_r(h)$   \n",
    "$h_{map} = argmax_{h \\in H}[ log P_r(h | D) + log P_r(h)]$  \n",
    "$h_{map} = argmin_{h \\in H}[ -log P_r(h | D) - log P_r(h)]$   \n",
    "\n",
    "\n",
    "- **Information theory**: The optimal code for some event w with probability $P_r$ has a length of $− log P_r$   \n",
    "- This means that in order to maximize the Maximum a Posteriori hypothesis, we need to minimize two terms that can be described as length:\n",
    "    - $log P_r(h) \\rightarrow$ This is the length of the hypothesis, which is the number of bits needed to represent this hypothesis (size of h)\n",
    "    - $log P_r(D | h) \\rightarrow$ This is the length of the data given a particular hypothesis. If the hypothesis perfectly describes the data, so we don’t need any data. But if the hypothesis labels some points wrong, so we need the correct labels for these points to be able to come up with a better hypothesis. So basically this term captures the error. (misclassification or error)\n",
    "- This is always a trade of, a more complex hypothesis will drive down error, while a simple hypothesis will have some error.\n",
    "- Find simplest hypotheis that explains the data and minimizes error.\n",
    "- We need to find the best hypothesis is the simplest hypothesis that minimizes error, which is called the Minimum Description.\n",
    "- For ANN, if weights get big will need more bits to express those big weights.  Bigger weights lead to overfitting.  \n",
    "- Complexity is not in the number of parameters directly but the values you need to represent those parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Classification\n",
    "---\n",
    "- The question in classification is “What is the best label?” not the best hypothesis.\n",
    "- To find the best label, we need to do a weighted vote for every single hypothesis in the hypotheses set, where the weight is the probability $P_r (h | D)$.\n",
    "- Now we end up trying to maximize $v_{map}$   \n",
    "\n",
    "**Bayes Optimal Classifier**   \n",
    "$V_{map} = argmax_{v_i \\in V} \\sum_{h_j \\in H} P_r(v_i | h_j) P_r(h_j | D)$\n",
    "\n",
    "- The Bayes optimal classifier is computationally very costly. This is because the posterior probability $P_r (h | D)$ must be computed for each hypothesis $h \\in H$ and combined with the prediction $P_r (v | h)$ before $V_{map}$ can be computed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Inference\n",
    "Representing and Reasoning With Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Independence\n",
    "---\n",
    "**Conditional Independence**: x is conditionally independent of y given z if the probability distribution\n",
    "governing x is independent of the value of y given the value of z:  \n",
    "\n",
    "$$ \\forall x, y, z \\rightarrow  P_r (X = x | Y = y, Z = z) = P_r (X = x | Z = z)$$   \n",
    " \n",
    "More compactly:   \n",
    "$$P_r (X | Y, Z) = P_r (X | Z)$$   \n",
    "\n",
    "This means that x is conditionally independent of y given z.  \n",
    "This comes originally from normal Independence and Chain Rule:  \n",
    "$$P_r (X, Y) = P_r (X) P_r (Y)$$\n",
    "$$P_r (X, Y) = P_r (X | Y) P_r (Y)$$\n",
    "$$P_r (X | Y) = P_r (X)$$   \n",
    "\n",
    "Normal Independence:  \n",
    "$P_r(x,y) = P_r(x)P_r(y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Belief Networks (Bayes Nets, Bayesian Networks, Graphical Models)\n",
    "---\n",
    "- A representation for probabilistic quantitates over complex spaces. \n",
    "- It’s a graphical representation of the **conditional independence** between all the variables in a joint distribution, with nodes corresponding to the variables and edges corresponding to the dependencies.  \n",
    "\n",
    "\n",
    "- Computations grow exponentially with adding more edges (dependencies).\n",
    "- A dependency relationship between two variables doesn’t mean a causal relationship.\n",
    "- A Belief Network must have a topological order. We can’t have cyclic relationships (two-way dependencies) (must be acyclic).\n",
    "- Parents of a node are the nodes immediate predecessors\n",
    "- Calculating independent probabilities from the graph is called sampling.\n",
    "- Does not show causal relationship.  Shows relationship between probabilities.  Represents conditional independencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recovering the Joint Distribution\n",
    "---\n",
    "<img src=\"../images/joint_dist.png\" width=500 align=\"right\"/>  \n",
    "- The Joint Probability of the graph is equal to the product of the probabilities of the variables in the graph.   \n",
    "    $P_r(A, B, C, D) = P_r(A) P_r(B) P_r(C|A,B) P_r(D|B,C) P_r(E|C,D)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "---\n",
    "- Calculating independent probabilities of variables in a distribution from the graph is called sampling.\n",
    "- Why sampling from a distribution is useful?\n",
    "    - Simulation of a complex process.\n",
    "    - Approximate inference: What might happen given some conditions? (machine)\n",
    "        - exact is hard, approximate is faster\n",
    "    - Facilitates visualizing the information provided by data.  (human)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferencing Rules\n",
    "----\n",
    "- Marginalization   \n",
    "$P_r(x) = \\sum_y P_r(x,y)$   \n",
    "\n",
    "- Chain Rule   \n",
    "$P_r(x,y) = P_r(x) P_r(y|x)$    \n",
    "\n",
    "- Bayes Rule   \n",
    "$P_r(y|x) = \\frac {P_r(x|y) P_r(y)} {P_r(x)}$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "---\n",
    "- Naïve Bayes classifiers are classifiers that represent a special case of the belief networks, but with stronger independence assumptions. For our classifier to be a naive Bayes classifier, we make the naïve **assumption that every attribute variable is conditionally independent of every other attribute variable**.\n",
    "- For the classification variable V, we would like to find the most probable target value $V_{map}$, given the values for attributes $(a_1, a_2, ... . , a_n)$. We can write the expression for $V_{map}$ and then use Bayes theorem to manipulate the expression as follows:\n",
    "\n",
    "$P_r(V|a_1, a_2, ...., a_n) = \\prod_i {P_r(a_i|V) P_r(V) / Z}$  \n",
    "MAP class $V_{map} = argmax P_r(V) \\prod_i P_r(a_i|V)$  \n",
    "\n",
    "- Why Naïve Bayes is useful?\n",
    "    - Inference is cheap: Each of the terms to be estimated is a one dimensional probability, which can be estimated with a smaller data set than the joint probability.\n",
    "    - Few parameters: The total number of terms to be estimated is the number of attributes n multiplied by the number of distinct values that v can take.\n",
    "    - We can estimate the parameters with labeled data.\n",
    "    - Connects inference and classification.\n",
    "    - Empirically successful and can handle missing attribtes.\n",
    "    - Tractible\n",
    "    - Gold standard\n",
    "    - Inference in any direction\n",
    "    - Can handle missing attributes\n",
    "- Disadvantages:\n",
    "    - Because of the strong conditional independence assumption placed on the attributes in the model, Naïve Bayes doesn’t model the inner relationships between attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/naive_bayes1.png\" width=600 align=\"left\"/>  \n",
    "<img src=\"../images/naive_bayes2.png\" width=600 align=\"left\"/>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
