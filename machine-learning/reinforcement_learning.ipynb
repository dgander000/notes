{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Decision Process\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the problem:  \n",
    "- States: S\n",
    "- Model: T(s,a,s') ~ Pr(s' | s,a)\n",
    "    - transition model - rules of game you are playing\n",
    "    - probability you will go to state s' given you are state s and take action a\n",
    "- Actions: A(s), A\n",
    "    - all actions agent can take\n",
    "- Reward: R(s), R(s,a), R(s,a,s')\n",
    "    - scalar value for being in a state\n",
    "    - domain knowledge\n",
    "    - reward you get for a state tells you the usefulness of that state\n",
    "    \n",
    "Solution:  \n",
    "- Policy: $\\pi(s) \\rightarrow a$\n",
    "    - takes state and tell you action you should take\n",
    "    - solution to MDP\n",
    "    - $\\pi^*$: optimal policy - maximizes long term reward\n",
    "\n",
    "Markov property   \n",
    "- only the present matters (can make sure current state remembers everything you need to remember from the past)\n",
    "- system is stationary - rules don't change - transition model doesn't change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rewards\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- delayed rewards\n",
    "- minor changes matter\n",
    "    - small negative reward for each step encourages you to end the game\n",
    "    - too large a negative reward and you may be rewarded to go to a bad state\n",
    "    - too large of a positive reward an you may be rewarded to not move or end game\n",
    "\n",
    "Temporal Credit Assignment - given sequence of <s,a,r> determine good/bad actions   \n",
    "\n",
    "Sequence of Rewards: Assumptions:  \n",
    "- infinite horizons (stationary)\n",
    "    - policy may change if finite horizon (only a few timesteps left)\n",
    "    - $\\pi(s, t) \\rightarrow a$\n",
    "- utility of sequences (stationary of preferences)\n",
    "    - if $U(s_0, s_1, s_2 ...) > U(s_0, s_1^{\\prime}, s_2^{\\prime} ...)$ the $U(s_1, s_2 ...) > U(s_1^{\\prime}, s_2^{\\prime} ...)$\n",
    "\n",
    "What doesn't work:  \n",
    "$U(s_0, s_1, s_2 ...) = \\sum_{t=0}^{\\infty}R(s_t)$ - Doesn't work - any type of reward will go to infinty (if rewards are positive) \n",
    "\n",
    "What does work (discounted rewards):   \n",
    "$U(s_0, s_1, s_2 ...) = \\sum_{t=0}^{\\infty}\\gamma^t R(s_t)$ where $0 \\leq \\gamma < 1$   \n",
    "bounded from above by largest award: $\\leq \\sum_{t=0}^{\\infty} \\gamma^t R_{max} = \\dfrac{R_{max}}{1-\\gamma}$ (geometric series)   \n",
    "allows us to add infinite number of numbers but gives a finite number   \n",
    "like having a finite horizon but it's always the same distance away (effectively infinite or unbounded)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policies\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\pi^* = argmax_{\\pi} E[\\sum_{t=0}^{\\infty} \\gamma^t R(s_t) | \\pi]$   \n",
    "$U^{\\pi}(s) = E[\\sum_{t=0}^{\\infty} \\gamma^t R(s_t) | \\pi, s_0=s]$   \n",
    "utility is long term feedback (accounts for delayed rewards) - does not equal R(S) which is immediate reward   \n",
    "$\\pi^* = argmax_a \\sum_{s^{\\prime}} T(s,a,s^{\\prime}) U(s^{\\prime}) \\rightarrow U(s)$ from following optimal policy   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bellman Equation   \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$U(s) = R(s) + \\gamma * max_a \\sum_{s^{\\prime}} T(s,a,s^{\\prime}) U(s^{\\prime})$    \n",
    "n equations in n unknows but max is non-linear   \n",
    "\n",
    "Finding policies:  \n",
    "- start with arbitrary utilities\n",
    "- update utilites based on neighbors\n",
    "- repeat until convergence  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$U_{t+1}(s) = R(s) + \\gamma * max_a \\sum_{s^{\\prime}} T(s,a,s^{\\prime}) U_t(s^{\\prime})$    \n",
    "R(s) is truth that keep getting added in  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- start with $\\pi_0$ guess\n",
    "- evaluate: given $\\pi_t$ calculate $U_t = U^{\\pi_t}$\n",
    "- improve: $\\pi_{t+1} = argmax_a \\sum T(s,a,s') U_t(s')$  \n",
    "\n",
    "$U_t(s) = R(s) + \\gamma \\sum_{s'} T(s,\\pi_t(s),s') U_t(s')$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL 'API'\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- planning\n",
    "    - model (T,R) -> Planner -> policy ($/pi$)\n",
    "- reinforcement learning\n",
    "    - transitions <s,a,r,s'> -> Learner -> policy\n",
    "- modeling\n",
    "    - transitions -> Modeler -> model\n",
    "- simulating\n",
    "    - model -> simulator -> transitions\n",
    "    \n",
    "reward maximization  \n",
    "\n",
    "RL-based planner\n",
    "- model -> simulator -> transitions -> learner -> policy\n",
    "model-based RL\n",
    "- transitions -> modeler -> planner -> policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Three Approaches to RL\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- policy search\n",
    "    - s -> $\\pi$ -> a\n",
    "    - direct use\n",
    "    - indirect learning\n",
    "- value-function based\n",
    "    - s -> U -> v\n",
    "- model-based\n",
    "    - <s,a> -> [T,R] -> <s',r>\n",
    "    - direct learning\n",
    "    - indirect use\n",
    "    - can use supervised learning\n",
    "    \n",
    "<img src=\"images/approaches_to_rl.png\" width=600 align=\"left\"/>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q Function (new kind of value function)\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Q(s,a) = R(s) + \\gamma \\sum_{s'} T(s,a,s') * max_{a'} Q(s',a')$  \n",
    "value for arriving in s, leaving via a, proceeding optimally thereafter   \n",
    "evaluating the Bellman equations from data:   <s,a,r,s'> -> Q   \n",
    "\n",
    "$U(s) = max_a (Q(s,a))$  \n",
    "$\\pi(s) = argmax_a (Q(s,a))$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating Q from transitions\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Q(s,a) = R(s) + \\gamma \\sum_{s'} T(s,a,s') * max_{a'} Q(s',a')$  \n",
    "\n",
    "but don't have [T, R], have transitions  \n",
    "learn to solve an MDP, without [T,R], interact <s,a,r,s'>   \n",
    "\n",
    "<s,a,r,s'>:   \n",
    "- $\\hat{Q}(s,a) \\leftarrow^{\\alpha_t} r + \\gamma * max_{a'} \\hat{Q}(s',a')$  \n",
    "- learning rate $\\alpha$  \n",
    "\n",
    "<img src=\"images/q_learning_convergence.png\" width=600 align=\"left\"/>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing Actions\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-learning is a family of algorithms\n",
    "- how to initialize Q\n",
    "- how to decay alpha\n",
    "- how to choose actions\n",
    "    - always choose same action (won't learn)\n",
    "    - choose randomly (won't use it)\n",
    "    - use Q (will use it) - greedy - local min\n",
    "    - random restarts (slow)\n",
    "    - epsilon greedy - \"simulated annealing\" like approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\epsilon$-Greedy Exploration\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If GLIE (greedy limit + infinite exploration) (decayed $\\epsilon$)   \n",
    "$\\hat{Q} \\rightarrow Q$ and $\\hat{\\pi} \\rightarrow \\pi^*$    \n",
    "\n",
    "learn - use  \n",
    "Exploration-Explotation dilema  (only one of you) (fundamental tradeoff in RL)  \n",
    "\n",
    "RL is glue between model learning (ML) and planning (automated planning and scheduling)   \n",
    "\n",
    "<img src=\"images/exploration.png\" width=600 align=\"left\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
